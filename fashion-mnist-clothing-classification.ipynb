{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-05T21:20:26.372338Z","iopub.execute_input":"2022-01-05T21:20:26.372823Z","iopub.status.idle":"2022-01-05T21:20:26.399783Z","shell.execute_reply.started":"2022-01-05T21:20:26.372720Z","shell.execute_reply":"2022-01-05T21:20:26.399101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# We load the data here","metadata":{}},{"cell_type":"code","source":"# example of loading the fashion mnist dataset\nfrom matplotlib import pyplot\nfrom keras.datasets import fashion_mnist","metadata":{"execution":{"iopub.status.busy":"2022-01-05T21:20:26.401054Z","iopub.execute_input":"2022-01-05T21:20:26.401383Z","iopub.status.idle":"2022-01-05T21:20:32.183702Z","shell.execute_reply.started":"2022-01-05T21:20:26.401340Z","shell.execute_reply":"2022-01-05T21:20:32.182781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# We Split the data into training and testing","metadata":{}},{"cell_type":"code","source":"# load dataset\n(trainX, trainy), (testX, testy) = fashion_mnist.load_data()\n# summarize loaded dataset\nprint('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\nprint('Test: X=%s, y=%s' % (testX.shape, testy.shape))","metadata":{"execution":{"iopub.status.busy":"2022-01-05T21:20:32.184837Z","iopub.execute_input":"2022-01-05T21:20:32.185076Z","iopub.status.idle":"2022-01-05T21:20:34.029157Z","shell.execute_reply.started":"2022-01-05T21:20:32.185047Z","shell.execute_reply":"2022-01-05T21:20:34.028265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# We get a glimpse of how our data looks like","metadata":{}},{"cell_type":"code","source":"# plot first few images\nfor i in range(9):\n    pyplot.subplot(330 + 1 + i)\n    pyplot.imshow(trainX[i], cmap=pyplot.get_cmap('gray'))\n# show the figure\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-05T21:20:34.031864Z","iopub.execute_input":"2022-01-05T21:20:34.032251Z","iopub.status.idle":"2022-01-05T21:20:34.812400Z","shell.execute_reply.started":"2022-01-05T21:20:34.032207Z","shell.execute_reply":"2022-01-05T21:20:34.810097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Different Aspects of the fashion dataset","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n# reshape dataset to have a single channel\ntrainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\ntestX = testX.reshape((testX.shape[0], 28, 28, 1))\n\n# one hot encode target values\ntrainy = to_categorical(trainy)\ntesty = to_categorical(testy)","metadata":{"execution":{"iopub.status.busy":"2022-01-05T21:20:34.813499Z","iopub.execute_input":"2022-01-05T21:20:34.815321Z","iopub.status.idle":"2022-01-05T21:20:35.194689Z","shell.execute_reply.started":"2022-01-05T21:20:34.815280Z","shell.execute_reply":"2022-01-05T21:20:35.193741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# After encoding thte values we cast them into floats for further processing","metadata":{}},{"cell_type":"code","source":"# convert from integers to floats\ntrain_norm = trainX.astype('float32')\ntest_norm = testX.astype('float32')\n# normalize to range 0-1\ntrain_norm = train_norm / 255.0\ntest_norm = test_norm / 255.0\n\ntrain_norm_y = trainy.astype('float32')\ntest_norm_y = testy.astype('float32')\n# normalize to range 0-1\ntrain_norm_yy = train_norm_y / 255.0\ntest_norm_yy = test_norm_y / 255.0","metadata":{"execution":{"iopub.status.busy":"2022-01-05T21:20:49.221443Z","iopub.execute_input":"2022-01-05T21:20:49.221711Z","iopub.status.idle":"2022-01-05T21:20:49.358867Z","shell.execute_reply.started":"2022-01-05T21:20:49.221682Z","shell.execute_reply":"2022-01-05T21:20:49.357781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# We prepare the pixels in array form","metadata":{}},{"cell_type":"code","source":"# scale pixels\ndef prep_pixels(train, test):\n    train_norm = train.astype('float32')\n    test_norm = test.astype('float32')\n    train_norm = train_norm / 255.0\n    test_norm = test_norm / 255.0\n    return train_norm, test_norm","metadata":{"execution":{"iopub.status.busy":"2022-01-05T21:22:14.367435Z","iopub.execute_input":"2022-01-05T21:22:14.367723Z","iopub.status.idle":"2022-01-05T21:22:14.372280Z","shell.execute_reply.started":"2022-01-05T21:22:14.367692Z","shell.execute_reply":"2022-01-05T21:22:14.371675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prep_pixels(train_norm, train_norm_y);","metadata":{"execution":{"iopub.status.busy":"2022-01-05T21:22:25.418467Z","iopub.execute_input":"2022-01-05T21:22:25.419141Z","iopub.status.idle":"2022-01-05T21:22:25.578871Z","shell.execute_reply.started":"2022-01-05T21:22:25.419103Z","shell.execute_reply":"2022-01-05T21:22:25.578105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# pixels done","metadata":{}},{"cell_type":"markdown","source":"# We import accuracy metrics and modelling algorithms of Deep learning","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom keras.datasets import fashion_mnist\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom tensorflow.keras.optimizers import SGD","metadata":{"execution":{"iopub.status.busy":"2022-01-05T21:22:28.165931Z","iopub.execute_input":"2022-01-05T21:22:28.166206Z","iopub.status.idle":"2022-01-05T21:22:28.855435Z","shell.execute_reply.started":"2022-01-05T21:22:28.166175Z","shell.execute_reply":"2022-01-05T21:22:28.854609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# We define our model with 5 layers and softmax activiation and he_uniform intializer to ensure our model trains all fitted values","metadata":{}},{"cell_type":"code","source":"def define_model():\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(10, activation='softmax'))\n    # compile model\n    opt = SGD(learning_rate=0.01, momentum=0.9)\n    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-01-05T21:23:08.232872Z","iopub.execute_input":"2022-01-05T21:23:08.233298Z","iopub.status.idle":"2022-01-05T21:23:08.240289Z","shell.execute_reply.started":"2022-01-05T21:23:08.233250Z","shell.execute_reply":"2022-01-05T21:23:08.239655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# After defining our loss function and optimizer as categorical_crossentropy and opt as its fashion dataset and categorical encoding needs to be cross checked for the gradient to be accurate","metadata":{}},{"cell_type":"code","source":"define_model()","metadata":{"execution":{"iopub.status.busy":"2022-01-05T21:23:08.985598Z","iopub.execute_input":"2022-01-05T21:23:08.986431Z","iopub.status.idle":"2022-01-05T21:23:09.032213Z","shell.execute_reply.started":"2022-01-05T21:23:08.986389Z","shell.execute_reply":"2022-01-05T21:23:09.031576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# We Now Evaluate our model","metadata":{}},{"cell_type":"code","source":"def evaluate_model(dataX, dataY, n_folds=5):\n    scores, histories = list(), list()\n    # prepare cross validation\n    kfold = KFold(n_folds, shuffle=True, random_state=1)\n    # enumerate splits\n    for train_ix, test_ix in kfold.split(dataX):\n        # define model\n        model = define_model()\n        # select rows for train and test\n        trainX, trainY, testX, testY = dataX[train_ix], dataY[train_ix], dataX[test_ix], dataY[test_ix]\n        # fit model\n        history = model.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX, testY), verbose=0)\n        # evaluate model\n        _, acc = model.evaluate(testX, testY, verbose=0)\n        print('> %.3f' % (acc * 100.0))\n        # append scores\n        scores.append(acc)\n        histories.append(history)\n    return scores, histories","metadata":{"execution":{"iopub.status.busy":"2022-01-05T21:24:03.243035Z","iopub.execute_input":"2022-01-05T21:24:03.243511Z","iopub.status.idle":"2022-01-05T21:24:03.252242Z","shell.execute_reply.started":"2022-01-05T21:24:03.243478Z","shell.execute_reply":"2022-01-05T21:24:03.251521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(train_norm, train_norm_y)","metadata":{"execution":{"iopub.status.busy":"2022-01-05T21:24:04.210049Z","iopub.execute_input":"2022-01-05T21:24:04.210756Z","iopub.status.idle":"2022-01-05T21:32:18.792169Z","shell.execute_reply.started":"2022-01-05T21:24:04.210715Z","shell.execute_reply":"2022-01-05T21:32:18.790931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# As shown we achieved 90% above accuracy due to data being completely sorted and minium use of computing power required\n* We could have used other loss fucntions for better performances\n* Used gradient Descent\n* transformed our data to greater extents before testing and training splits","metadata":{}}]}